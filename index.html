<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
    <!-- Replace the content tag with appropriate information -->
    <meta name="description" content="DESCRIPTION META TAG">
    <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
    <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
    <meta property="og:url" content="URL OF THE WEBSITE"/>
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
    <meta property="og:image" content="static/image/your_banner_image.png"/>
    <meta property="og:image:width" content="1200"/>
    <meta property="og:image:height" content="630"/>


    <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
    <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
    <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
    <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
    <meta name="twitter:card" content="summary_large_image">
    <!-- Keywords for your paper to be indexed by-->
    <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>TUMTraf V2X Cooperative Perception Dataset</title>
    <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/bulma-slider.min.js"></script>
    <script src="static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-4 publication-title">TUMTraf V2X Cooperative Perception Dataset</h1>
                    <div class="is-size-5 publication-authors">
                            <span class="author-block"><a href="https://walzimmer.github.io/website/"
                                                          target="_blank">Walter Zimmer</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://www.linkedin.com/in/gerhard-arya-wardana/"
                                                      target="_blank">Gerhard Arya Wardana</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://suren3141.github.io/" target="_blank">Suren
                                    Sritharan</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://www.linkedin.com/in/xingcheng-zhou-64b0a6189"
                                                      target="_blank">Xingcheng Zhou</a><sup>1</sup>,</span>
                        <span class="author-block"><a href="https://rruisong.github.io/" target="_blank">Rui
                                    Song</a><sup>1,2</sup>,</span>
                        <span class="author-block"><a
                                href="https://www.ce.cit.tum.de/air/people/prof-dr-ing-habil-alois-knoll"
                                target="_blank">Alois C. Knoll</a><sup>1</sup></span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Technical University of Munich,</span>
                        <span class="author-block"><sup>2</sup>Fraunhofer IVI</span>
                        <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                                <span class="link-block">
                              <a href="https://arxiv.org/pdf/2403.01316.pdf" target="_blank"
                                 class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                <i class="fas fa-file-pdf"></i>
                                </span>
                                <span>Paper</span>
                              </a>
                            </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/tum-traffic-dataset/coopdet3d" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Model</span>
                                    </a>
                                </span>


                            <!-- Dataset link -->
                            <span class="link-block">
                                    <a href="https://innovation-mobility.com/en/project-providentia/a9-dataset/#anchor_release_4"
                                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fa fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/walzimmer/3d-bat" target="_blank"
                                       class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Labeling Tool</span>
                                    </a>
                                </span>

                            <!-- Github link -->
                            <span class="link-block">
                                    <a href="https://github.com/tum-traffic-dataset/tum-traffic-dataset-dev-kit"
                                       target="_blank" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Development Kit</span>
                                    </a>
                                </span>


                            <!-- ArXiv abstract Link -->
                            <!-- <span class="link-block">
                          <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                            <i class="ai ai-arxiv"></i>
                          </span>
                          <span>arXiv</span>
                          </a>
                        </span> -->
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero is-small">
    <div class="hero-body">
        <div class="container">
            <!--            <h2 class="title is-3">Dataset Visualization</h2>-->
            <div id="results-carousel" class="carousel results-carousel">
                <div class="item item-video5">
                    <video poster="" id="video5" autoplay controls muted loop height="100%">
                        <source src="static/videos/drive_15_labels_visualization_small.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="item item-video6">
                    <video poster="" id="video6" autoplay controls muted loop height="100%">
                        <source src="static/videos/drive_22_labels_visualization_small.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="item item-video7">
                    <video poster="" id="video7" autoplay controls muted loop height="100%">
                        <source src="static/videos/drive_41_labels_visualization_small.mp4" type="video/mp4">
                    </video>
                </div>
                <div class="item item-video8">
                    <video poster="" id="video8" autoplay controls muted loop height="100%">
                        <source src="static/videos/drive_42_labels_visualization_small.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Overview  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Overview</h2>
                <div class="content">
                    <strong>TUMTraf-V2X</strong> is the first high-quality real-world <strong>V2X dataset</strong>
                    for the
                    cooperative 3D object detection and tracking task in autonomous driving.<br><br>
                    It contains:
                    <ul>
                        <li>data collected by <strong>9 sensors</strong> simultaneously from onboard and roadside
                            sensors.
                        </li>
                        <li><strong>2,000</strong> labeled point clouds and <strong>5,000</strong> labeled images.
                        </li>
                        <li><strong>30k</strong> 3D bounding boxes with track IDs.</li>
                        <li>Challenging scenarios: near-miss and <strong>traffic violation events</strong>,
                            overtaking
                            and U-turn maneuvers.
                        </li>
                        <li><strong>HD maps</strong> of the driving domains.</li>
                        <li>Labels in <strong>OpenLABEL</strong> standard.</li>
                        <li>A <strong>dev kit</strong> to load, preprocess, visualize, convert labels, and to
                            evaluate
                            perception methods.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End overview -->

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p>
                        Cooperative perception offers several benefits for enhancing the capabilities of autonomous
                        vehicles and improving road safety. Using roadside sensors in addition to onboard sensors
                        increases reliability and extends the sensor range. They offer a higher situational
                        awareness
                        for automated vehicles and prevent occlusions. We propose CoopDet3D, a cooperative
                        multi-modal
                        fusion model, and TUMTraf-V2X, a perception dataset, for the cooperative 3D object detection
                        and tracking
                        task. Our dataset contains 2,000 labeled point clouds and 5,000 labeled images from five
                        roadside and four onboard sensors. It includes 30k 3D boxes with track IDs and precise GPS
                        and
                        IMU data. We labeled eight categories and covered occlusion scenarios with challenging
                        driving
                        maneuvers, like traffic violations, near-miss events, overtaking, and U-turns. Through
                        multiple
                        experiments, we show that our CoopDet3D camera-LiDAR fusion model achieves an increase of
                        +14.36
                        3D mAP compared to a vehicle camera-LiDAR fusion model. Finally, we make our model, dataset,
                        labeling tool, and development kit publicly available to advance in connected and automated
                        driving.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Sensor setup  -->
<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns">
            <div class="column is-four-fifths">
                <h2 class="title is-3">Sensor Setup</h2>
                <div class="content is-align-content-start">
                    On the <strong>infrastructure</strong>, the following roadside sensors were used:
                    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <ul>
                        <li>1x <strong>Ouster</strong> LiDAR OS1-64 (generation 2), 64 vertical layers, 360° FOV,
                            <br>below
                            horizon configuration, 10 cm accuracy @120 m range
                        </li>
                        <li>4x <strong>Basler</strong> ace acA1920-50gc, 1920×1200, Sony IMX174 with 8 mm lenses
                        </li>
                    </ul>
                    <br>On the <strong>vehicle</strong>, the following onboard sensors were
                    used:<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                    <ul>
                        <li>1x <strong>Robosense</strong> RS-LiDAR-32, 32 vert. layers, 360° FOV, 3 cm accuracy @200
                            m
                            range
                        </li>
                        <li>1x <strong>Basler</strong> ace acA1920-50gc, 1920×1200, Sony IMX174 with 16 mm lens</li>
                        <li>1x <strong>Emlid</strong> Reach RS2+ multi-band RTK GNSS receiver</li>
                        <li>1x <strong>XSENS</strong> MTi-30-2A8G4 IMU</li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- End sensor setup -->

<!-- infrastructure sensors -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <div class="item">
                &nbsp;
                <img src="static/images/infrastructure_sensors.jpg" alt="infrastructure_sensors"/>
                <h2 class="subtitle has-text-centered">
                    Visualization of roadside sensors used to record the TUMTraf-V2X Cooperative Perception Dataset
                    from
                    infrastructure perspective.
                </h2>
            </div>
        </div>
    </div>
</section>


<section class="hero is-small is-light">
    <div class="hero-body">
        <div class="container">
            <h2 class="title is-3">Dataset Labeling</h2>
            <video poster="" autoplay controls muted loop height="100%">
                <source src="static/videos/timelapse_labeling_fast.mp4" type="video/mp4">
            </video>
        </div>
    </div>
</section>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop is-centered has-text-centered">
            <h2 class="title">Dataset Statistics</h2>
            <div id="results-carousel2" class="carousel results-carousel">
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/bar_chart_class_occurrences_all_drives.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Distribution of objects between day and night.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/bar_chart_avg_num_points_per_class_all_drives.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Average and maximum number of 3D points for each class.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/bar_chart_avg_track_lengths_all_drives_log.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Average and maximum track length for all classes.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/histogram_avg_rotation.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Visualization of rotations (yaw).
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/line_chart_avg_num_points_with_distance.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        3D points grouped by distance.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/bev_plot_all_drives.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        BEV visualization of tracks.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/histogram_num_objects_per_num_points.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of 3D points.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/histogram_objects_in_frame.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of 3D box labels.
                    </h2>
                </div>
                <div class="item">
                    <!-- Your image here -->
                    <img src="static/images/histogram_track_lengths.jpg" alt="MY ALT TEXT"/>
                    <h2 class="subtitle has-text-centered">
                        Histogram of track lengths.
                    </h2>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="hero">
    <div class="hero">
        <h3 class="title is-8 has-text-centered">Benchmark</h3>
        <div class="container is-centered">
            <table class="table is-centered center">
                <thead>
                <tr>
                    <th>Config</th>
                    <th></th>
                    <th>BEV mAP</th>
                    <th></th>
                    <th>3D mAP</th>
                    <th></th>
                    <th></th>
                </tr>
                <tr>
                    <th>Domain</th>
                    <th>Modality</th>
                    <th>&nbsp;</th>
                    <th>Easy</th>
                    <th>Moderate</th>
                    <th>Hard</th>
                    <th>Average</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>Vehicle</td>
                    <td>Camera</td>
                    <td>46.83</td>
                    <td>31.47</td>
                    <td>37.82</td>
                    <td>30.77</td>
                    <td>30.36</td>
                </tr>
                <tr>
                    <td>Vehicle</td>
                    <td>LiDAR</td>
                    <td>85.33</td>
                    <td>85.22</td>
                    <td>76.86</td>
                    <td>69.04</td>
                    <td>80.11</td>
                </tr>
                <tr>
                    <td>Vehicle</td>
                    <td>Camera + LiDAR</td>
                    <td>84.90</td>
                    <td>77.60</td>
                    <td>72.08</td>
                    <td>73.12</td>
                    <td>76.40</td>
                </tr>
                <tr>
                    <td>Infrastructure</td>
                    <td>Camera</td>
                    <td>61.98</td>
                    <td>31.19</td>
                    <td>46.73</td>
                    <td>40.42</td>
                    <td>35.04</td>
                </tr>
                <tr>
                    <td>Infrastructure</td>
                    <td>LiDAR</td>
                    <td>92.86</td>
                    <td>86.17</td>
                    <td>88.07</td>
                    <td>75.73</td>
                    <td>84.88</td>
                </tr>
                <tr>
                    <td>Infrastructure</td>
                    <td>Camera + LiDAR</td>
                    <td>92.92</td>
                    <td>87.99</td>
                    <td><strong>89.09</strong></td>
                    <td><strong>81.69</strong></td>
                    <td><u>87.01</u></td>
                </tr>
                <tr>
                    <td>Cooperative</td>
                    <td>Camera</td>
                    <td>68.94</td>
                    <td>45.41</td>
                    <td>42.76</td>
                    <td>57.83</td>
                    <td>45.74</td>
                </tr>
                <tr>
                    <td>Cooperative</td>
                    <td>LiDAR</td>
                    <td><u>93.93</u></td>
                    <td><u>92.63</u></td>
                    <td>78.06</td>
                    <td>73.95</td>
                    <td>85.86</td>
                </tr>
                <tr>
                    <td>Cooperative</td>
                    <td>Camera + LiDAR</td>
                    <td><strong>94.22</strong></td>
                    <td><strong>93.42</strong></td>
                    <td><u>88.17</u></td>
                    <td><u>79.94</u></td>
                    <td><strong>90.76</strong></td>
                </tr>
                </tbody>
            </table>
            <h5 class="is-3 has-text-centered">
                Evaluation results (BEV mAP and 3D mAP) of <strong>CoopDet3D</strong> on our <br>
                TUMTraf-V2X Cooperative Perception test set in south2 FOV.
            </h5>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgements</h2>
        <div class="content has-text-justified">
            <p>Our <a href="https://github.com/tum-traffic-dataset/coopdet3d">CoopDet3D</a> model is build on top of
                <a href="https://github.com/mit-han-lab/bevfusion">BEVFusion</a> and <a
                        href="https://ieeexplore.ieee.org/abstract/document/9921947">PillarGrid</a>.
                This research was supported by the Federal Ministry of Education and Research in Germany within the
                <a
                        href="https://www.ika.rwth-aachen.de/en/competences/projects/automated-driving/autotech-agil-en.html">AUTOtech.agil</a>
                project, Grant Number: 01IS22088U.
            </p>
        </div>


    </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{zimmer2024tumtraf,
    title={TUMTraf V2X Cooperative Perception Dataset},
    author={Zimmer, Walter and Wardana, Gerhard Arya and Sritharan, Suren and Zhou, Xingcheng and Song, Rui and Knoll, Alois},
    journal={arXiv preprint arXiv:2403.01316},
    year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content has-text-centered">
                    <p>
                        The <strong>TUMTraf-V2X</strong> Cooperative Perception Dataset is licensed under <a
                            href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA
                        4.0</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

<!-- End of Statcounter Code -->

</body>

</html>